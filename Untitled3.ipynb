{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlitedict as sqld\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('TestProject001/sample_news_2019-03-04/sample_news_2019-03-04_compressed.npz') as npz:\n",
    "    ids = npz['ids']\n",
    "    sents = npz['sents']\n",
    "    embs = npz['embs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = sqld.SqliteDict('test.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ids)):\n",
    "    id_map[int(ids[i])] = f'{sents[i]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Online tools such as Smart Investor have valueResearch furthers your money educationBeing afraid of money\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_map[ids[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"doesn't help\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_map[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0424 12:26:33.504294 4591781312 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import os.path as p\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sqlitedict as sqld\n",
    "from typing import List, Tuple, Union\n",
    "from copy import deepcopy\n",
    "\n",
    "from SimSent.vectorizer.sentence_vectorizer import SentenceVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /Users/lukasferrer/Documents/SimSent/SimSent/vectorizer/model/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/\n",
      "Initializing TF Session...\n"
     ]
    }
   ],
   "source": [
    "sv = SentenceVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexBuilder:\n",
    "    # Return Types\n",
    "    MMAP_ARRAYS = Tuple[np.array, np.array, np.array]\n",
    "    BASE_INDEX = faiss.Index\n",
    "    \n",
    "    def __init__(self, project_dir: Path, \n",
    "                sentence_vectorizer: object = None, large_encoder: bool = False):\n",
    "        self.project_dir = project_dir\n",
    "        self.sub_dir = None\n",
    "        self.seed_name = None\n",
    "\n",
    "        if sentence_vectorizer:\n",
    "            self.sv = sentence_vectorizer\n",
    "        else:\n",
    "            self.sv = SentenceVectorizer(large=large_encoder)\n",
    "\n",
    "    def tsv_to_index(self, dump_tsv: Union[str, Path]):\n",
    "        f_name = Path(dump_tsv).stem\n",
    "        self.sub_dir = self.project_dir/f_name\n",
    "        self.seed_name = f_name\n",
    "        os.makedirs(p.abspath(self.sub_dir), exist_ok=True)\n",
    "\n",
    "        # Vectorize to npz\n",
    "        npz_name = self.sub_dir/f'{f_name}_compressed.npz'\n",
    "        if not p.exists(npz_name):\n",
    "            self.sv.prep_npz(input_tsv=dump_tsv, output_npz=npz_name)\n",
    "\n",
    "        # Load as mmap arrays\n",
    "        ids, embs, sents = self.load_npz(npz_name)\n",
    "            \n",
    "        # Prepare base index & write on-disk index\n",
    "        base_index = self.train_base_index(embeddings=embs)  \n",
    "        self.make_mmap_index(base_index,\n",
    "                             embs=embs, ids=ids)\n",
    "\n",
    "        # Get id-to-sent mapping\n",
    "        self.populate_db(ids=ids, sents=sents)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_npz(npz_name: Union[str, Path]) -> MMAP_ARRAYS:\n",
    "        with np.load(npz_name, mmap_mode='r') as npz:\n",
    "            ids = npz['ids']\n",
    "            embs = npz['embs']\n",
    "            sents = npz['sents']\n",
    "        return ids, embs, sents\n",
    "    \n",
    "    def train_base_index(self, embeddings: np.array,\n",
    "        n_centroids: int = 1024, compression: str = 'Flat'     # Or Flat\n",
    "        ) -> BASE_INDEX:\n",
    "\n",
    "        idx_type = f'IVF{n_centroids},{compression}'\n",
    "        base_idx_pth = p.abspath(self.sub_dir/f'{idx_type}_base.index')\n",
    "\n",
    "        if p.exists(base_idx_pth):\n",
    "            index = faiss.read_index(base_idx_pth)\n",
    "        else:\n",
    "            index = faiss.index_factory(embeddings.shape[1], idx_type)\n",
    "            index.train(embeddings)\n",
    "            faiss.write_index(index, base_idx_pth)\n",
    "        return index\n",
    "\n",
    "    def make_mmap_index(self, base_index: BASE_INDEX,\n",
    "                        ids: np.array, embs: np.array):\n",
    "\n",
    "        # Get invlists\n",
    "        index = faiss.clone_index(base_index)\n",
    "        index.add_with_ids(embs, ids)\n",
    "        ivf_vector = faiss.InvertedListsPtrVector()\n",
    "        ivf_vector.push_back(index.invlists)\n",
    "        index.own_invlists = False\n",
    "        del index\n",
    "        gc.collect()\n",
    "\n",
    "        # Make MMAP ivfdata\n",
    "        index_name = p.abspath(self.sub_dir/f'{self.seed_name}_mmap')\n",
    "        invlists = faiss.OnDiskInvertedLists(base_index.nlist, \n",
    "                                             base_index.code_size, \n",
    "                                             f'{index_name}.ivfdata')\n",
    "        ntotal = invlists.merge_from(ivf_vector.data(), ivf_vector.size())\n",
    "\n",
    "        # Link index to ivfdata and save\n",
    "        index = faiss.clone_index(base_index)\n",
    "        index.ntotal = ntotal\n",
    "        index.replace_invlists(invlists)\n",
    "        faiss.write_index(index, f'{index_name}.index')\n",
    "        \n",
    "    # TODO: REWORK THIS\n",
    "    def populate_db(self, ids: np.array, sents: np.array):\n",
    "        db_file = p.abspath(self.sub_dir/f'{self.seed_name}_id-sent-map.sqlite')\n",
    "        id_to_sent = sqld.SqliteDict(db_file, autocommit=True)\n",
    "\n",
    "        for i in range(ids.shape[0]):\n",
    "            id_to_sent[str(ids[i])] = str(sents[i])\n",
    "\n",
    "        id_to_sent.close()\n",
    "\n",
    "#     def prep_index(self, dump_tsv: Path):\n",
    "#         f_name = dump_tsv.stem\n",
    "#         sub_dir = self.project_dir/f_name\n",
    "#         os.makedirs(p.abspath(sub_dir), exist_ok=True)\n",
    "\n",
    "#         # Vectorize\n",
    "#         npz_name = self.project_dir/f_name/f'{f_name}_compressed.npz'\n",
    "#         if not p.exists(npz_name):\n",
    "#             self.sv.prep_npz(input_tsv=dump_tsv, output_npz=npz_name)\n",
    "        \n",
    "#         print('Finished vectorization')\n",
    "        \n",
    "#         # Load\n",
    "#         with np.load(npz_name) as npz:\n",
    "#             ids = npz['ids']\n",
    "#             embs = npz['embs']\n",
    "#             sents = npz['sents']\n",
    "\n",
    "#         n_centroids = 1024      # TODO: choose wrt len(embs) \n",
    "#         if not p.exists(sub_dir/f'{f_name}_{n_centroids}SQ8_base.index'):\n",
    "#             # Train                 # TODO: Put in func\n",
    "#             idx_type = f'IVF{n_centroids},SQ8'\n",
    "#             index = faiss.index_factory(embs.shape[1], idx_type)\n",
    "#             index.train(embs)\n",
    "#             faiss.write_index(index, p.abspath(sub_dir/f'{f_name}_{n_centroids}SQ8_base.index'))\n",
    "#         else:\n",
    "#             index = faiss.read_index(p.abspath(sub_dir/f'{f_name}_{n_centroids}SQ8_base.index'))\n",
    "        \n",
    "#         print('Finished training base idx')\n",
    "\n",
    "#         # Populate idx          # TODO: Put in func that saves several chunks if embs is too large\n",
    "#         index.add_with_ids(embs, ids)\n",
    "#         faiss.write_index(index, p.abspath(sub_dir/f'{f_name}_{n_centroids}SQ8_in-memory.index'))\n",
    "#         sub_idxs = [p.abspath(sub_dir/f'{f_name}_{n_centroids}SQ8_in-memory.index')]\n",
    "#         del index\n",
    "#         gc.collect()\n",
    "\n",
    "#         # Make on-disk index    # TODO: Put in func\n",
    "#         ivfs = list()\n",
    "#         for idx in sub_idxs:\n",
    "#             index = faiss.read_index(idx, faiss.IO_FLAG_MMAP)\n",
    "#             ivfs.append(index.invlists)\n",
    "#             index.own_invlists = False  # Prevents de-allocation\n",
    "#             del index\n",
    "#             gc.collect()\n",
    "\n",
    "#         index = faiss.read_index(p.abspath(sub_dir/f'{f_name}_{n_centroids}SQ8_base.index'))\n",
    "        \n",
    "#         # build inv list\n",
    "#         invlists = faiss.OnDiskInvertedLists(index.nlist, index.code_size,\n",
    "#                                              p.abspath(sub_dir/f'{f_name}_{n_centroids}SQ8_on-disk.ivfdata'))\n",
    "#         ivf_vector = faiss.InvertedListsPtrVector()\n",
    "#         for ivf in ivfs:\n",
    "#             ivf_vector.push_back(ivf)\n",
    "\n",
    "#         ntotal = invlists.merge_from(ivf_vector.data(), ivf_vector.size())\n",
    "#         index.ntotal = ntotal\n",
    "#         index.replace_invlists(invlists)\n",
    "\n",
    "#         faiss.write_index(index, p.abspath(sub_dir/f'{f_name}_{n_centroids}SQ8_on-disk.index'))\n",
    "            \n",
    "#         print('Finished writing on-disk idx')\n",
    "            \n",
    "#         # Populate db\n",
    "#         id_to_sent = sqld.SqliteDict(p.abspath(sub_dir/f'{f_name}_id-sent-map.db'))\n",
    "#         for i in range(len(ids)):\n",
    "#             id_to_sent[int(ids[i])] = f'{sents[i]}'\n",
    "\n",
    "#         print('Finished writing db')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = p.abspath('TestProject001/')\n",
    "idx_bdr = IndexBuilder(Path(project_dir), sentence_vectorizer=sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_bdr.tsv_to_index(Path('TestProject001/sample_news_2019-03-04.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_bdr.tsv_to_index(Path('TestProject001/sample_news_2019-03-05.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_bdr.tsv_to_index(Path('TestProject001/sample_news_2019-03-06.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in void faiss::OnDiskInvertedLists::do_mmap() at OnDiskInvertedLists.cpp:243: Error: 'f' failed: could not open /Users/lukasferrer/Documents/SimSent/Test2/test/test_1024SQ8_on-disk.ivfdata in mode r+: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-162eca73e626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test2/test2/test_1024SQ8_on-disk.index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in void faiss::OnDiskInvertedLists::do_mmap() at OnDiskInvertedLists.cpp:243: Error: 'f' failed: could not open /Users/lukasferrer/Documents/SimSent/Test2/test/test_1024SQ8_on-disk.ivfdata in mode r+: No such file or directory"
     ]
    }
   ],
   "source": [
    "idx = faiss.read_index('Test2/test2/test_1024SQ8_on-disk.index', faiss.IO_FLAG_ONDISK_SAME_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.03949273, 0.71097904, 0.71097904, 0.71097904, 0.872072  ]],\n",
       "       dtype=float32), array([[    1,  6307, 16216, 15801, 13477]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.search(embs[0:1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(idx, 'just,_checking.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2 = faiss.read_index('just_checking.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Path('lol/no.npz').stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SimSent.faiss_cache import faiss_cache\n",
    "\n",
    "__all__ = ['BaseIndexer', 'faiss_cache',\n",
    "           'DiffScores', 'VectorIDs', 'FaissSearch']\n",
    "\n",
    "\n",
    "DiffScores = List[List[np.float32]]\n",
    "VectorIDs = List[List[np.int64]]\n",
    "FaissSearch = Tuple[DiffScores, VectorIDs]\n",
    "\n",
    "\n",
    "class BaseIndexHandler(object):\n",
    "    def __init__(self):\n",
    "        self.index = None\n",
    "        self.dynamic = False\n",
    "        self.io_flag = faiss.IO_FLAG_ONDISK_SAME_DIR\n",
    "\n",
    "    @faiss_cache(128)\n",
    "    def search(self, query_vector: np.array, k: int) -> FaissSearch:\n",
    "        return self.index.search(query_vector, k)\n",
    "\n",
    "    def get_index_paths(self, idx_dir_pth: Path, \n",
    "                        nested: bool = False) -> List[str]:\n",
    "        if nested:\n",
    "            get = '*/*.index'\n",
    "        else:\n",
    "            get = '*.index'\n",
    "\n",
    "        index_paths = glob.glob(p.abspath(idx_dir_pth/get))\n",
    "        index_paths = [Path(pth) for pth in index_paths if \n",
    "                       faiss.read_index(pth, self.io_flag).ntotal > 0]\n",
    "\n",
    "        return sorted(index_paths)\n",
    "\n",
    "    @staticmethod\n",
    "    def joint_sort(scores: DiffScores, ids: VectorIDs) -> FaissSearch:\n",
    "        \"\"\"\n",
    "        Sorts scores in ascending order while maintaining score::id mapping.\n",
    "        Checks if input is already sorted.\n",
    "        :param scores: Faiss query/hit vector L2 distances\n",
    "        :param ids: Corresponding faiss vector ids\n",
    "        :return: Scores sorted in ascending order with corresponding ids\n",
    "        \"\"\"\n",
    "        # Possible nested lists\n",
    "#         if isinstance(scores[0], (list, array)):\n",
    "#             scores, ids = scores[0], ids[0]\n",
    "\n",
    "#         print(scores)\n",
    "#         print(ids)\n",
    "\n",
    "        # Check if sorted\n",
    "        if all(scores[i] <= scores[i + 1] for i in range(len(scores) - 1)):\n",
    "            return scores, ids\n",
    "\n",
    "        # Joint sort\n",
    "        sorted_difs, sorted_ids = (list(sorted_dif_ids) for sorted_dif_ids\n",
    "                                     in zip(*sorted(zip(scores, ids))))\n",
    "\n",
    "        return [sorted_difs], [sorted_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from multiprocessing import Pipe, Process, Queue\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "#### Parallelized Nearest Neighbor Search ####\n",
    "class Shard(Process):\n",
    "    def __init__(self, shard_name: str, shard_path: Path, \n",
    "                 input_pipe: Pipe, output_queue: Queue,\n",
    "                 nprobe: int = 4, daemon: bool = False):\n",
    "        \"\"\" RangeShards search worker \"\"\"\n",
    "        super().__init__(name=shard_name, daemon=daemon)\n",
    "        self.input = input_pipe\n",
    "        self.index = faiss.read_index(p.abspath(shard_path), \n",
    "                                      faiss.IO_FLAG_ONDISK_SAME_DIR)\n",
    "        self.index.nprobe = nprobe\n",
    "        self.output = output_queue\n",
    "\n",
    "    def run(self):\n",
    "        @faiss_cache(64)\n",
    "        def neighborhood(index, query, radius):\n",
    "            _, difs, ids = index.range_search(query, radius)\n",
    "            return difs, ids\n",
    "\n",
    "        if self.input.poll():\n",
    "            (query_vector, radius_limit) = self.input.recv()\n",
    "            difs, ids = neighborhood(self.index, query_vector, radius_limit)\n",
    "            self.output.put((self.name, difs, ids), block=False)\n",
    "\n",
    "\n",
    "class RangeShards(BaseIndexHandler):\n",
    "    def __init__(self, shard_dir: Union[str, Path], \n",
    "                 nprobe: int = 4, get_nested: bool = False):\n",
    "        \"\"\"\n",
    "        For deploying multiple, pre-made IVF indexes as shards.\n",
    "            (intended for on-disk indexes that do not fit in memory)\n",
    "\n",
    "        Note: The index shards must be true partitions with no overlapping ids\n",
    "\n",
    "        :param shard_dir: Dir containing faiss index shards\n",
    "        :param nprobe: Number of clusters to visit during search\n",
    "                       (speed accuracy trade-off)\n",
    "        :param get_nested: Load indexes in sub directories of shard_dir\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.paths_to_shards = self.get_index_paths(Path(shard_dir), \n",
    "                                                    nested=get_nested)\n",
    "        self.nprobe = nprobe\n",
    "        self.dynamic = True\n",
    "        self.lock = False\n",
    "\n",
    "        self.results = Queue()\n",
    "        self.shards = dict()\n",
    "        self.n_shards = 0\n",
    "        for shard_path in self.paths_to_shards:\n",
    "            self.load_shard(shard_path)\n",
    "        for shard_name, (handler_pipe, shard) in self.shards.items():\n",
    "            shard.start()\n",
    "            self.n_shards += 1\n",
    "\n",
    "    def load_shard(self, shard_path: Path):\n",
    "        shard_name = Path(shard_path).stem\n",
    "        shard_pipe, handler_pipe = Pipe(duplex=False)\n",
    "        shard = Shard(shard_name, shard_path,\n",
    "                      input_pipe=shard_pipe, \n",
    "                      output_queue=self.results,\n",
    "                      nprobe=self.nprobe, daemon=False)\n",
    "        self.shards[shard_name] = (handler_pipe, shard)\n",
    "\n",
    "    @faiss_cache(128)\n",
    "    def search(self, query_vector: np.array, keys: list, \n",
    "               radius: float = 1.0) -> FaissSearch:\n",
    "\n",
    "        if query_vector.shape[0] > 1:\n",
    "            query_vector = np.reshape(query_vector, (1, 512))\n",
    "\n",
    "        # Lock search while loading index or actively searching\n",
    "        while self.lock:\n",
    "            sleep(1)\n",
    "\n",
    "        # Lock out other searches\n",
    "        self.lock = True\n",
    "\n",
    "        # Start parallel range search\n",
    "        n_results = 0\n",
    "        for shard_name, (hpipe, shard) in self.shards.items():\n",
    "            if shard_name in keys:\n",
    "                hpipe.send((query_vector, radius))\n",
    "                shard.run()\n",
    "                n_results += 1\n",
    "\n",
    "        # Aggregate results\n",
    "        results = dict()\n",
    "        while n_results > 0 or not self.results.empty():\n",
    "            name, difs, ids = self.results.get()\n",
    "            results[name] = self.joint_sort(difs, ids)\n",
    "            n_results -= 1 \n",
    "\n",
    "        self.lock = False\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RangeShards('TestProject001/', get_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_news_2019-03-04_mmap',\n",
       " 'sample_news_2019-03-05_mmap',\n",
       " 'sample_news_2019-03-06_mmap']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(rs.shards.keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rs.search(embs[1:2], keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 1), (0.6829833, 6829), (0.6829833, 16651), (0.6829833, 17086), (0.68332684, 704)]\n",
      "[(0.70729554, 12220), (0.79234034, 1425), (0.81015825, 12211), (0.81180775, 21319), (0.81778175, 1987)]\n",
      "[(0.7811464, 14096), (0.78185153, 14678), (0.78664434, 19446), (0.8098983, 19456), (0.8474065, 11525)]\n"
     ]
    }
   ],
   "source": [
    "for k, results in a.items():\n",
    "    scores, ids = results\n",
    "#     print(results)\n",
    "    print(sorted(zip(scores[0], ids[0]))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27764"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent = sqld.SqliteDict('Test7/test/test_id-sent-map.sqlite')\n",
    "len(id_to_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Online tools such as Smart Investor have valueResearch furthers your money educationBeing afraid of money'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine a doughnut representing the fees charged by your KiwiSaver fund.'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Event URL:\\n'], dtype='<U3875')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27764"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70252"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.shards['test_mmap'][1].index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83292"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "27764*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2],\n",
       "       [20095],\n",
       "       [42490]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(ids==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.floor(math.log(28764, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.pow(2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SimSent",
   "language": "python",
   "name": "simsent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
