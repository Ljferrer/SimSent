{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimSent.indexer.index_builder import IndexBuilder\n",
    "from SimSent.vectorizer.sentence_vectorizer import SentenceVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /Users/lukasferrer/Documents/SimSent/SimSent/vectorizer/model/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/\n",
      "Initializing TF Session...\n"
     ]
    }
   ],
   "source": [
    "sv = SentenceVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path('TestProject002/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibdr = IndexBuilder(project_dir, sentence_vectorizer=sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TestProject002/sample_news_2019-03-05.tsv',\n",
       " 'TestProject002/sample_news_2019-03-04.tsv',\n",
       " 'TestProject002/sample_news_2019-03-06.tsv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('TestProject002/*.tsv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    ibdr.tsv_to_index(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0424 16:24:33.640244 4672722368 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import os.path as p\n",
    "import glob\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from sqlitedict import SqliteDict\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SimSent.vectorizer.sentence_vectorizer import DockerVectorizer\n",
    "from SimSent.indexer.deploy_handler import RangeShards\n",
    "from SimSent.indexer.faiss_cache import faiss_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    QueryReturn = np.array\n",
    "    DiffScores = List[np.float32]\n",
    "    VectorIDs = List[np.int64]\n",
    "    FaissSearch = Tuple[DiffScores, VectorIDs]\n",
    "    FormattedSearch = List[Tuple[np.int64, np.float32, str]]\n",
    "    FormattedMultiSearch = Dict[str, FormattedSearch]\n",
    "\n",
    "    def __init__(self, query_vectorizer: object, index_handler: object, \n",
    "                 project_dir: Path, nested: bool = False):\n",
    "        super().__init__()\n",
    "        self.vectorizer = query_vectorizer\n",
    "        self.indexer = index_handler\n",
    "        \n",
    "        # Get id-to-sent maps\n",
    "        get = '*/*.sqlite' if nested else '*.sqlite'\n",
    "        db_files = glob.glob(p.abspath(project_dir/get))\n",
    "        self.sent_dbs = dict()\n",
    "        for f in db_files:\n",
    "            self.sent_dbs[Path(f).stem] = SqliteDict(f)\n",
    "\n",
    "    @faiss_cache(32)\n",
    "    def query_corpus(self, query_str: str, keys: List[str], \n",
    "                     k: int = 5, radius: float = 1.0, verbose: bool = True\n",
    "                     ) -> FormattedMultiSearch:\n",
    "        \"\"\"\n",
    "        Vectorize query -> Search faiss index handler -> Format doc payload.\n",
    "        Expects to receive only one query per call.\n",
    "        \"\"\"\n",
    "        # Vectorize\n",
    "        t_v = time()\n",
    "        query_vector = self.vectorize(query_str)\n",
    "\n",
    "        # Search\n",
    "        t_s = time()\n",
    "        results = self.indexer.search(query_vector, keys, radius=radius)\n",
    "\n",
    "        t_p = time()\n",
    "        top_hits = list()\n",
    "        similar_docs = dict()\n",
    "        for source, result_set in results.items():\n",
    "            sorted_set = self.format_results(source, result_set, k)\n",
    "            top_hits.extend(sorted_set)\n",
    "            similar_docs[source] = sorted_set\n",
    "        similar_docs['top_hits'] = sorted(top_hits)[:k]\n",
    "\n",
    "        t_r = time()\n",
    "        if verbose:\n",
    "            print(f'  Query vectorized in --- {t_s - t_v:0.4f}s')\n",
    "            print(f'  Index searched in ----- {t_p - t_s:0.4f}s')\n",
    "            print(f'  Payload formatted in -- {t_r - t_p:0.4f}s\\n')\n",
    "\n",
    "        return similar_docs\n",
    "\n",
    "    def vectorize(self, query: Union[str, List[str]]) -> QueryReturn:\n",
    "        \"\"\"\n",
    "        Use DockerVectorizer for fast Query Vectorization.\n",
    "        :param query: Text to vectorize\n",
    "        :return: Formatted query embedding\n",
    "        \"\"\"\n",
    "        if not isinstance(query, list):\n",
    "            query = [query]\n",
    "        if len(query) > 1:\n",
    "            query = query[:1]\n",
    "\n",
    "        query_vector = self.vectorizer.make_vectors(query)\n",
    "\n",
    "        if isinstance(query_vector[0], list):\n",
    "            query_vector = np.array(query_vector, dtype=np.float32)\n",
    "        return query_vector\n",
    "\n",
    "    def format_results(self, source: str, result_set: FaissSearch, k: int\n",
    "                       ) -> FormattedSearch:\n",
    "        scores, hit_ids = result_set\n",
    "        sents = list()\n",
    "        for sent_id in hit_ids:\n",
    "            sents.append(self.sent_dbs[source][str(sent_id)])\n",
    "        \n",
    "        return sorted(zip(scores, hit_ids, sents))[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DockerVectorizer()\n",
    "rs = RangeShards(project_dir, nprobe=32, get_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = QueryProcessor(dv, rs, project_dir=project_dir, nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_news_2019-03-04', 'sample_news_2019-03-05', 'sample_news_2019-03-06']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(rs.shards.keys())\n",
    "# keys = [keys[0].replace('_mmap', '_id-sent-map')]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Query vectorized in --- 0.0139s\n",
      "  Index searched in ----- 0.0217s\n",
      "  Payload formatted in -- 0.9474s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample_news_2019-03-04': [(0.44618067, 12448, 'GOOD)'),\n",
       "  (0.44618067, 12462, ': GOOD)'),\n",
       "  (0.44843227, 6714, '\"This has to change.'),\n",
       "  (0.5184382, 6750, 'Not So Fast!'),\n",
       "  (0.57668424, 8074, '\"It is great to be back.\\xa0 ')],\n",
       " 'sample_news_2019-03-05': [(0.38179156, 17447, 'This need not be so.'),\n",
       "  (0.45797464, 21663, 'This is simply absurd.'),\n",
       "  (0.46639562, 21146, \"Well I wouldn't go that far.\"),\n",
       "  (0.47100002, 20566, 'We probably should look at it.'),\n",
       "  (0.4732894, 20585, 'Obviously, there is a little way to go')],\n",
       " 'sample_news_2019-03-06': [(0.3150266,\n",
       "   15989,\n",
       "   \"It makes things interesting, and that's a good thing.\"),\n",
       "  (0.3150266, 16177, \"It makes things interesting, and that's a good thing.\"),\n",
       "  (0.35274342, 4748, \"It's a shame.\"),\n",
       "  (0.36258426, 13807, 'Even if it is a little soon.'),\n",
       "  (0.3919842, 6115, \"I mean, it's really...\")],\n",
       " 'top_hits': [(0.3150266,\n",
       "   15989,\n",
       "   \"It makes things interesting, and that's a good thing.\"),\n",
       "  (0.3150266, 16177, \"It makes things interesting, and that's a good thing.\"),\n",
       "  (0.35274342, 4748, \"It's a shame.\"),\n",
       "  (0.36258426, 13807, 'Even if it is a little soon.'),\n",
       "  (0.38179156, 17447, 'This need not be so.')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp.query_corpus('This is a good thing', keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp.sent_dbs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.shards[keys[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SimSent",
   "language": "python",
   "name": "simsent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
